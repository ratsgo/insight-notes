---
layout: default
title: Transformer
parent: Interpretable AI
has_children: true
permalink: /docs/interpretable/transformer
nav_order: 1
---

# Transformer

구글브레인에서 트랜스포머(transformer)라는 아키텍처를 제안한 [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) 논문이 나온지도 햇수로 3년의 세월이 흘렀습니다. 도전적인 주제(CNN, RNN 없이 기계 번역 태스크를 수행해보자), 다소 도발적인 제목(비틀즈 노래 패러디), 그리고 엄청난 성능으로 주목을 끌었던 논문인데요. 트랜스포머 블록을 기반으로 한 자연어 처리 모델(GPT, BERT, ALBERT...)이 다수 탄생하게 된 계기이기도 합니다. 
하지만 폭발적인 관심 대비 트랜스포머가 왜 잘 되는지, 내부 작동 원리는 아직 미지의 영역이라고 할 수 있는데요. 최근 들어 관련 연구 성과가 속속 나오고 있습니다. 이 항목에서는 트랜스포머가 잘 되는 이유에 대해 살펴보려고 합니다.
